{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MIWdWJ8v_Em"
   },
   "source": [
    "# NLP 실습 #2: N-gram Language model\n",
    "황순원의 '소나기' 로 'kenlm'을 이용하여 n-gram language model 학습하고 사용해보기\n",
    "</br>\n",
    "</br>\n",
    "* 준비: Kenlm 설치\n",
    "\n",
    "* Step 1: Data preprocessing - tokenization\n",
    "\n",
    "* Step 2: Language Model 생성 (Probability Table을 생성)\n",
    "\n",
    "* Step 3: Binary file로 Model 변환\n",
    "\n",
    "* Step 4: Model을 활용하여 시퀀스 확률 찾기\n",
    "\n",
    "* Step 5: 주어진 시퀀스에서 다음 형태소 예측\n",
    "\n",
    "* Step 6: 주어진 시퀀스로 시작하는 문장을 예측\n",
    "\n",
    "* Step 7: 다른 말뭉치로 학습한 모델과 비교\n",
    "\n",
    "</br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u58Vf5yByd-j"
   },
   "source": [
    "## 준비: konlpy와 kenlm library 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home2/jyoh96/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "from konlpy.tag import Kkma\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8sMydXeJivy7",
    "outputId": "b41fb390-9262-433d-f249-fc57a5565d72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이', '문장', '이', '형태소', '단위', '로', '잘', '출력', '되', '나요', '?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. konlpy의 Kkma 형태소 분석기 예제\n",
    "\n",
    "tokenizer = Kkma()\n",
    "tokenizer.morphs(\"이 문장이 형태소 단위로 잘 출력되나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UxOsfjyKLaJ2",
    "outputId": "99112d08-8475-49f3-f5c6-197f9c9c1bab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['입력이 문장 단위로 잘 구분되는지 확인해보세요.',\n",
       " 'sent_tokenize는 한 line을 받아서 마침표를 기준으로 여러 문장으로 분리해주는 함수입니다!',\n",
       " '이 문장은 세번째 문장입니다.',\n",
       " '이 문장은 네번째 문장입니다!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. nltk의 tokenizer 예제 \n",
    "\n",
    "sent_tokenize('입력이 문장 단위로 잘 구분되는지 확인해보세요. sent_tokenize는 한 line을 받아서 마침표를 기준으로 여러 문장으로 분리해주는 함수입니다! 이 문장은 세번째 문장입니다. 이 문장은 네번째 문장입니다!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LcMwd5ZlLj22",
    "outputId": "9c0bdbb5-0b88-42ed-afbf-4dfa8755ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-13 18:54:01--  https://kheafield.com/code/kenlm.tar.gz\n",
      "Resolving kheafield.com (kheafield.com)... 35.196.63.85\n",
      "Connecting to kheafield.com (kheafield.com)|35.196.63.85|:443... connected.\n",
      "ERROR: cannot verify kheafield.com's certificate, issued by ‘CN=R3,O=Let's Encrypt,C=US’:\n",
      "  Issued certificate has expired.\n",
      "To connect to kheafield.com insecurely, use `--no-check-certificate'.\n",
      "\n",
      "gzip: stdin: unexpected end of file\n",
      "tar: Child returned status 1\n",
      "tar: Error is not recoverable: exiting now\n",
      "/home2/jyoh96/ch2-ngram_lm/kenlm/build/kenlm/build\n",
      "CMake Error: The source directory \"/home2/jyoh96/ch2-ngram_lm/kenlm/build/kenlm\" does not appear to contain CMakeLists.txt.\n",
      "Specify --help for usage, or press the help button on the CMake GUI.\n",
      "make: *** No targets specified and no makefile found.  Stop.\n",
      "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
      "  Using cached https://github.com/kpu/kenlm/archive/master.zip\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home2/jyoh96/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "[Errno 2] No such file or directory: '/home/jyoh96/ch2-ngram_lm'\n",
      "/home2/jyoh96/ch2-ngram_lm/kenlm/build/kenlm/build\n"
     ]
    }
   ],
   "source": [
    "# 3. kenlm 설치\n",
    "\n",
    "!wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
    "!mkdir -p ./kenlm/build\n",
    "%cd ./kenlm/build\n",
    "!cmake ..\n",
    "!make -j 4\n",
    "!pip install https://github.com/kpu/kenlm/archive/master.zip\n",
    "%cd /home/jyoh96/ch2-ngram_lm\n",
    "#%cd /home/ch2_ngram_language_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THyoh5vyyvTE"
   },
   "source": [
    "## Step 1: Data preprocessing - tokenization\n",
    "&emsp;&emsp; sonagi.doc의 내용을 문장 단위로 분리.\n",
    "\n",
    "</br> &emsp;&emsp; 각 문장을 kkma 분석기로 형태소 단위로 분리.\n",
    "\n",
    "</br> &emsp;&emsp; 형태소로 분석된 문장들과 vocab들을 sonagi.txt, sonagi.voc 파일로 작성.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkaozFjwLqAF"
   },
   "outputs": [],
   "source": [
    "# 4. sonagi.doc을 읽어들임. \n",
    "\n",
    "lines = []\n",
    "with open('sonagi.doc', 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6MrcQOMNTM-",
    "outputId": "7bb9f3b9-ef4e-43a9-b1c8-dbef99c4b1ce"
   },
   "outputs": [],
   "source": [
    "# 5. nltk의 sent_tokenize를 이용하여 sonagi.doc의 내용을 문장 단위로 분리\n",
    "sentences = []\n",
    "for line in lines:\n",
    "    sentences.extend(sent_tokenize(line))\n",
    "    \n",
    "print(sentences[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kcpOrgRCO-IN",
    "outputId": "06e9c7ee-ba6c-4e8f-9eee-e64d8ee409f7"
   },
   "outputs": [],
   "source": [
    "# 6. Kkma tokenizer로 sentences 내용을 형태소 단위로 분리\n",
    "\n",
    "morphs_lst = []\n",
    "for sentence in sentences:\n",
    "    temp = tokenizer.morphs(sentence)\n",
    "    morphs_lst.append(temp)\n",
    "\n",
    "print(morphs_lst[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_EZOns9Immuw",
    "outputId": "c0bb5032-5590-4c8d-92fd-f7ff83e33826"
   },
   "outputs": [],
   "source": [
    "# 7. 형태소 분석된 결과물 확인\n",
    "\n",
    "print(sentences[0])\n",
    "print(morphs_lst[0])\n",
    "print()\n",
    "print(sentences[1])\n",
    "print(morphs_lst[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KysxZCltLwC_"
   },
   "outputs": [],
   "source": [
    "# 8. 형태소 분석된 각 sentence들을 한 줄에 한 문장씩 sonagi.txt 파일로 작성\n",
    "\n",
    "with open('sonagi.txt', 'wt', encoding='utf-8') as f:\n",
    "    for morphs in morphs_lst:\n",
    "        f.write(\" \".join(morphs).strip())\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19N303W_LxBJ",
    "outputId": "1f824560-484b-48bd-d766-dfaa5fa4beaa"
   },
   "outputs": [],
   "source": [
    "# 9. 형태소 분석된 각 sentence의 형태소들을 전부 set에 집어넣어 중복을 삭제, set에 들어있는 vocab을 한 줄에 한 형태소씩 .voc 파일로 작성\n",
    "\n",
    "vocab = set()\n",
    "for morphs in morphs_lst:\n",
    "    for t in morphs:\n",
    "        vocab.add(t)\n",
    "\n",
    "print(vocab)\n",
    "\n",
    "with open('sonagi.voc', 'wt', encoding='utf-8') as f:\n",
    "    for line in vocab:\n",
    "        f.write(line.strip())\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "au2RwXvElH4T",
    "outputId": "87113120-ce8e-4a2d-b2a4-fa55e2bd6754"
   },
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h63hHQfLy6xp"
   },
   "source": [
    "## Step 2: Language model 생성\n",
    "\n",
    "\n",
    "### &emsp;>> ! kenlm/build/bin/lmplz -o [N] < [txt file] > [model file]\n",
    "&emsp;&emsp;[N] : 사용할 모델의 n-gram\n",
    "\n",
    "</br> &emsp;&emsp;[txt file] : 학습(counting)에 사용할 문서 경로\n",
    "\n",
    "</br> &emsp;&emsp;[model file] : 생성할 모델 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3g_yzHeLzbs",
    "outputId": "adf62792-dedc-485c-8740-03e0aff6cbaa"
   },
   "outputs": [],
   "source": [
    "# 10. kenlm 모델 build\n",
    "\n",
    "! kenlm/build/bin/lmplz -o 3 <sonagi.txt> sonagi.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C77ePn8uRcp2",
    "outputId": "81a2b7d7-6a52-4bd9-b219-ed528caf6e6c"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cozxk_yzJI2"
   },
   "source": [
    "## Step 3: Binary file로 Model 변환\n",
    "### &emsp;>> ! kenlm/build/bin/build_binary [model file] [bin model file]\n",
    "\n",
    "&emsp;&emsp;[N] : n-gram\n",
    "\n",
    "</br>&emsp;&emsp;[model file] : 이미 생성된 모델 파일명\n",
    "\n",
    "</br>&emsp;&emsp;[bin model file] : binary로 생성할 모델 파일명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VLk8DGA5L1dQ",
    "outputId": "9a6f3f1c-12fd-416f-e89b-52a050d42ecd"
   },
   "outputs": [],
   "source": [
    "# 11. 생성된 kenlm 모델 파일 binary  file로 변환\n",
    "\n",
    "! kenlm/build/bin/build_binary sonagi.arpa sonagi.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CxDftsRfL1_d",
    "outputId": "9fd309eb-7f2c-4140-adfa-7b01634811b0"
   },
   "outputs": [],
   "source": [
    "# 12. model 이 제대로 생성 되었는지 확인\n",
    "# ! echo \"원하는 문장\" | kenlm/build/bin/query [(bin) model file]\n",
    "\n",
    "! echo \"소녀 가 소년 을\" | kenlm/build/bin/query sonagi.bin\n",
    "! echo \"소녀 가 소년 을\" | kenlm/build/bin/query sonagi.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUsVg6IhzToE"
   },
   "source": [
    "## Step 4: model을 활용하여 주어진 시퀀스 scoring\n",
    "\n",
    "### &emsp;>> model.score('여름 이 었 다 . ', bos=True, eos=True)\n",
    "### &emsp;>> model.score('여름 이 었 ', bos=True, eos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aohuLOD2L3E-"
   },
   "outputs": [],
   "source": [
    "# 13. 생성한 모델 불러오기\n",
    "\n",
    "model_file = 'sonagi.bin' \n",
    "# model_file = 'sonagi.arpa' # arpa 모델도 사용가능하나, binary 파일이 속도가 좀더 빠름.\n",
    "\n",
    "model = kenlm.Model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pK9QiuqUL4Ao",
    "outputId": "ef133fee-a50e-4c3a-fbd3-34548eb0e600"
   },
   "outputs": [],
   "source": [
    "# 14. 시퀀스 scoring 예시\n",
    "\n",
    "# model.score('주어진 시퀀스', bos=True, eos=True)\n",
    "# bos : 모델이 scoring하도록 주어진 시퀀스 내에서 sentence가 시작했는가\n",
    "# eos : 모델이 scoring하도록 주어진 시퀀스 내에서 sentence가 종결되었는가\n",
    "\n",
    "# 이미 완성한 시퀀스(문장)의 scoring을 위해서는 bos=True eos=True\n",
    "# bos=True로 하면 모델이 시퀀스의 앞에 <s>를 추가하고,\n",
    "# eos=True인 경우 모델이 시퀀스의 뒤에 </s>를 추가하여 score를 계산함\n",
    "\n",
    "#따라서 bos=True, eos=True는 '<s> 원하는 시퀀스 </s>' 의 scoring을 한 것임.\n",
    "\n",
    "\n",
    "#  \"<s> 여름 이 었 다 . </s>\" 를 scoring'\n",
    "eos_score = model.score('여름 이 었 다 .', bos=True, eos=True)\n",
    "print(\"<s> 여름 이 었 다 . </s> :\", eos_score)\n",
    "\n",
    "#  \"<s> 여름 이 었\" 을 scoring\n",
    "no_eos_score = model.score('여름 이 었', bos=True, eos=False)\n",
    "print(\"<s> 여름 이 었 :\", no_eos_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7EcGs1h3xXv"
   },
   "source": [
    "## Step 5: 주어진 시퀀스에서 다음 형태소 예측\n",
    "\n",
    "### &emsp;<< Exercise 1>> '소년 은 소녀 를' 다음에 올 가장 자연스러운 형태소를 찾아주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URqEQ5LkL55H"
   },
   "outputs": [],
   "source": [
    "# 15. vocab list 파일 불러오기\n",
    "\n",
    "vocab_file = 'sonagi.voc'\n",
    "vocab_list = []\n",
    "with open(vocab_file, 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        vocab_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZpoQfp8L6oS",
    "outputId": "500f1b30-3d19-4ed7-e446-8cdb20860d41"
   },
   "outputs": [],
   "source": [
    "###########################################################################################################################\n",
    "# 16. << Exercise 1 >> 주어진 시퀀스('소년 은 소녀 를') 다음의 형태소를 예측 : 주어진 시퀀스에 voc list의 형태소 하나를 합쳐서 score를 계산\n",
    "\n",
    "# 주어진 시퀀스에 preprocessing하며 만든 sonagi.voc 파일의 형태소를 하나씩 붙여서 가장 score가 높은 형태소를 찾음\n",
    "# voc = ['가', '만', '사업', ...] 이라면,\n",
    "#'소년 은 소녀 가' 의 점수, '소년 은 소녀 만', '소년 은 소녀 사업 ', ... 등의 score를 계산하고 그 중 가장 높은 score를 지닌 형태소를 찾음\n",
    "############################################################################################################################\n",
    "\n",
    "target = \"소년 은 소녀 를\"\n",
    "    \n",
    "best_token = None\n",
    "best_score = None\n",
    "\n",
    "for vocab in vocab_list:\n",
    "    sequence_cand = target + \" \" + vocab\n",
    "    score_cand = model.score(sequence_cand, bos=True, eos=False)\n",
    "\n",
    "    if best_vocab is None:\n",
    "        best_score = score_cand\n",
    "        best_vocab = vocab\n",
    "\n",
    "    if best_score < score_cand:\n",
    "        best_score = score_cand\n",
    "        best_vocab = vocab\n",
    "\n",
    "sequence_cand = target\n",
    "score_cand = model.score(sequence_cand, bos=True, eos=True)\n",
    "\n",
    "if best_score < score_cand:\n",
    "    best_score = score_cand\n",
    "    best_vocab = '</s>'\n",
    "\n",
    "print(best_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdmfWMpe4MhX"
   },
   "source": [
    "## Step 6: 주어진 시퀀스로 시작하는 가장 자연스러운 문장 예측\n",
    "\n",
    "#### </br>&emsp;<< Exercise 2 >> Step 5와 loop문을 사용하여,\n",
    "#### </br> &emsp;'소년 은 소녀 를' 으로 시작하는 가장 자연스러운 문장을 찾아주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iyc4XiqUL8cA"
   },
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "# 17. << Exercise 2 >> 16를 auto-regressive하게 loop 문으로 반복하여 주어진 시퀀스 '소년 은 소녀 를' 로 시작한 문장이 어떻게 끝날지 예측해주세요.\n",
    "\n",
    "# 1) 주어진 시퀀스에서 문장이 종결될 경우의 score를 계산\n",
    "# 2) 주어진 시퀀스에서 문장이 종결되지 않을 경우에 그 다음에 올 가장 자연스러운 형태소와 score를 계산\n",
    "# 3) 만약 1)보다 2)의 score가 더 높다면, 시퀀스에 2)에서 찾아낸 형태소를 추가하고, 다시 1)부터 반복\n",
    "# 4) 만약 1)의 score가 2)의 score 보다 더 높다면, 그대로 시퀀스를 마무리 함\n",
    "# find_next_token 함수를 반드시 쓸 필요는 없습니다!\n",
    "###############################################################################################################################\n",
    "\n",
    "def find_full_sentence(partial_sentence, model_path, voc_path):\n",
    "\n",
    "    # load language model with KenLM\n",
    "    model = kenlm.Model(model_path)\n",
    "    print(\"Language Model at \" + model_path + \" loaded.\")\n",
    "\n",
    "    # load vocabulary (or load word list)\n",
    "    voc = []\n",
    "    file_reader = open(voc_path, \"r\", encoding='utf-8')\n",
    "    for line in file_reader:\n",
    "        voc.append(line.strip())\n",
    "    print(\"Vocabulary at \" + voc_path + \" loaded\\n\")\n",
    "\n",
    "    # prediction loop\n",
    "    loop_condition = True\n",
    "    while loop_condition:\n",
    "        best_score = None\n",
    "        best_vocab = None\n",
    "\n",
    "        for vocab in voc:\n",
    "            sequence_cand = sequence + \" \" + vocab\n",
    "            sequence_score = model.score(sequence_cand, bos=True, eos=False)\n",
    "\n",
    "            if best_score is None:\n",
    "                best_score = sequence_score\n",
    "                best_vocab = vocab\n",
    "\n",
    "            if best_score < sequence_score:\n",
    "                best_score = sequence_score\n",
    "                best_vocab = vocab\n",
    "\n",
    "        eos_sentence = sequence\n",
    "        eos_score = model.score(eos_sentence, bos=True, eos=True)\n",
    "        \n",
    "        if eos_score < best_score:\n",
    "            sequence = sequence + \" \" + best_vocab\n",
    "      \n",
    "        else:\n",
    "            loop_condition = False\n",
    "\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9M1VzEIL8fw",
    "outputId": "3177027b-1525-46cd-e8f2-6f93784a4eb1"
   },
   "outputs": [],
   "source": [
    "# 17. 16을 실행 \n",
    "\n",
    "target = input(\"Sequence: \")\n",
    "print(\"Given sequence: \" + target)\n",
    "\n",
    "model_path = 'sonagi.arpa'\n",
    "voc_path = 'sonagi.voc'\n",
    "print(find_full_sentence(target, model_path, voc_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELatjR6K4zzB"
   },
   "source": [
    "## Step 7: 다른 말뭉치로 학습한 모델과 비교\n",
    "\n",
    "#### </br> &emsp;<< Exercise 3 >> Step 1, 2, 3, 4, 6을 참고하여, \n",
    "#### </br> &emsp;bucketwheat_flowers.doc를 사용한 모델을 생성하여 \n",
    "#### </br> &emsp;주어진 시퀀스('소년 은 소녀 를', '개울가 가', '여름 에')로 시작하는 가장 자연스러운 문장들을 찾고, \n",
    "#### </br> &emsp;sonagi.doc를 사용한 경우와 비교해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOBIYmOhL_dY",
    "outputId": "b8cb552c-02c1-4511-8a0c-a76a31cd2a09"
   },
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "# 18. << Exercise 4 >> : bucketwheat_flowers.doc를 사용한 모델을 생성하여 sonagi.doc를 사용한 경우와 주어진 시퀀스에 대한 output을 비교해주세요.\n",
    "# 주어진 시퀀스: '소년 은 소녀 를', '개울가 가', '여름 에'\n",
    "\n",
    "# sonata.doc를 data preprocessing하여 sonata.txt, sonata.voc를 생성.\n",
    "# sonata.txt, sonata.voc를 이용하여 모델을 생성, load하여서\n",
    "# 같은 문장에 대해서 sonagi corpus를 이용한 경우와 sonata corpus를 이용한 경우에 output이 어떠한지 출력\n",
    "####################################################################################################################\n",
    "\n",
    "# Data preprocessing. 형태소 분하여 buckwheat_flowers.txt, buckwheat_flowers.voc 작성\n",
    "b_lines = []\n",
    "with open('buckwheat_flowers.doc', 'rt', encoding='utf-8') as b_f:\n",
    "    for b_line in b_f:\n",
    "        b_lines.append(b_line)\n",
    "\n",
    "b_sentences = []\n",
    "for line in b_lines:\n",
    "    b_sentences.extend(sent_tokenize(line))\n",
    "\n",
    "b_morph_sentences = []\n",
    "for sentence in b_sentences:\n",
    "    temp = \"\"\n",
    "    for morpheme in tokenizer.morphs(sentence):\n",
    "      temp = temp + \" \" + morpheme\n",
    "    b_morph_sentences.append(temp.strip())\n",
    "\n",
    "with open('buckwheat_flowers.txt', 'wt', encoding='utf-8') as f:\n",
    "    for line in b_morph_sentences:\n",
    "        f.write(line.strip())\n",
    "        f.write('\\n')\n",
    "\n",
    "b_vocab = set()\n",
    "for sentence in b_morph_sentences:\n",
    "    temp = sentence.split()\n",
    "    for t in temp:\n",
    "        b_vocab.add(t)\n",
    "\n",
    "with open('buckwheat_flowers.voc', 'wt', encoding='utf-8') as f:\n",
    "    for line in b_vocab:\n",
    "        f.write(line.strip())\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "# n-gram language model을 buckwheat_flowers로 학습\n",
    "\n",
    "! kenlm/build/bin/lmplz -o 3 <buckwheat_flowers.txt> buckwheat_flowers.arpa\n",
    "! kenlm/build/bin/build_binary buckwheat_flowers.arpa buckwheat_flowers.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IADqh60wtMp2",
    "outputId": "11cf0903-c47e-48c0-9452-c7140a6bd942"
   },
   "outputs": [],
   "source": [
    "# 19. 18을 실행\n",
    "target = input(\"Sequence: \")\n",
    "print(\"Given sequence: \" + target)\n",
    "\n",
    "model_path2 = 'buckwheat_flowers.arpa'\n",
    "voc_path2 = 'buckwheat_flowers.voc'\n",
    "print(find_full_sentence(target, model_path, voc_path))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "practice_konlpy_kenlm.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
